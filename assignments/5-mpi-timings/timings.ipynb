{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is meant to be run on **four nodes** with 28 cores.\n",
    "\n",
    "One thing we are trying to do is gauge the performance of a good MPI implementation and compare it to a good OpenMP implementation.  In order to do this, we will have to load some slightly different modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =========================================================================\n",
      "|                                                                         |\n",
      "|       A note about python/3.6:                                          |\n",
      "|       PACE is lacking the staff to install all of the python 3          |\n",
      "|       modules, but we do maintain an anaconda distribution for          |\n",
      "|       both python 2 and python 3. As conda significantly reduces        |\n",
      "|       the overhead with package management, we would much prefer        |\n",
      "|       to maintain python 3 through anaconda.                            |\n",
      "|                                                                         |\n",
      "|       All pace installed modules are visible via the module avail       |\n",
      "|       command.                                                          |\n",
      "|                                                                         |\n",
      " =========================================================================\n",
      "Currently Loaded Modulefiles:\n",
      "  1) curl/7.42.1\n",
      "  2) git/2.13.4\n",
      "  3) python/3.6\n",
      "  4) /nv/coc-ice/tisaac3/opt/pace-ice/modulefiles/jupyter/1.0\n",
      "  5) cuda/9.1\n",
      "  6) /nv/coc-ice/tisaac3/opt/pace-ice/modulefiles/gcc/9.2.0-offload\n",
      "  7) /nv/coc-ice/tisaac3/opt/pace-ice/modulefiles/hpctoolkit/2018.18-gcc\n",
      "  8) hwloc/1.10.0(default)\n",
      "  9) /nv/coc-ice/tisaac3/opt/pace-ice/modulefiles/mvapich2/2.3-gcc\n",
      " 10) cse6230/gcc-omp-gpu\n"
     ]
    }
   ],
   "source": [
    "module unload cse6230\n",
    "module load cse6230/gcc-omp-gpu\n",
    "module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =========================================================================\n",
      "|                                                                         |\n",
      "|       A note about python/3.6:                                          |\n",
      "|       PACE is lacking the staff to install all of the python 3          |\n",
      "|       modules, but we do maintain an anaconda distribution for          |\n",
      "|       both python 2 and python 3. As conda significantly reduces        |\n",
      "|       the overhead with package management, we would much prefer        |\n",
      "|       to maintain python 3 through anaconda.                            |\n",
      "|                                                                         |\n",
      "|       All pace installed modules are visible via the module avail       |\n",
      "|       command.                                                          |\n",
      "|                                                                         |\n",
      " =========================================================================\n"
     ]
    }
   ],
   "source": [
    "module load cse6230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([mvapich](http://mvapich.cse.ohio-state.edu/) is a fork of the [mpich](https://www.mpich.org/) MPI implementation with some modifications for performance on various HPC hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring MPI primitives\n",
    "\n",
    "**Task 1 (3 pts)** The file `benchmarks.c` includes some basic benchmarks for measuring the performance of various MPI point-to-point and collective operations.  Right now, it is incomplete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2p pingpong / broadcast pingpong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /* TODO: split the communicator `comm` into one communicator for ranks\n",
      "  /* TODO: destroy the subcommunicator created in `splitCommunicator` */\n",
      "  /* TODO: Record the MPI walltime in `tic_p` */\n",
      "  /* TODO: Get the elapsed MPI walltime since `tic_in`,\n",
      "  /* TODO: take the times from all processes and compute the maximum,\n"
     ]
    }
   ],
   "source": [
    "grep \"TODO\" benchmarks.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refering to a good [MPI Tutorial](https://computing.llnl.gov/tutorials/mpi/) or\n",
    "[lecture notes](http://vuduc.org/cse6230/slides/cse6230-fa14--06-mpi.pdf) as needed,\n",
    "fill in the missing MPI routines.\n",
    "\n",
    "Once you have done that, run the following script to generate a graph of benchmark bandwidths of MPI routines.  Note that these values are only for MPI messages within a node: values may be different when we start using multiple nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o benchmarks\n",
      "mpicc -fopenmp -I../../utils -g -Wall -O3 -std=c99 -c -o benchmarks.o benchmarks.c\n",
      "mpicc -fopenmp -o benchmarks benchmarks.o \n",
      "MPI Version: 3.1\n",
      "MVAPICH2 Version      :\t2.3.2\n",
      "MVAPICH2 Release date :\tFri August 9 22:00:00 EST 2019\n",
      "MVAPICH2 Device       :\tch3:mrail\n",
      "MVAPICH2 configure    :\t--prefix=/nv/coc-ice/tisaac3/opt/pace-ice/mvapich2/2.3 --with-hwloc --with-pbs=/opt/torque/current --enable-romio --with-file-system=ufs+nfs --enable-shared --enable-sharedlibs=gcc\n",
      "MVAPICH2 CC           :\tgcc    -DNDEBUG -DNVALGRIND -O2\n",
      "MVAPICH2 CXX          :\tg++   -DNDEBUG -DNVALGRIND -O2\n",
      "MVAPICH2 F77          :\tgfortran -L/lib -L/lib   -O2\n",
      "MVAPICH2 FC           :\tgfortran   -O2\n",
      "\n",
      "MPI # Procs: 56\n",
      "MPI Wtime 1.57125e+09, precision 1e-06\n",
      "MPI Wtime is global\n",
      "MPI proc 0 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 1 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 2 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 3 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 4 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 5 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 6 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 7 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 8 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 9 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 10 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 11 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 12 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 13 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 14 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 15 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 16 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 17 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 18 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 19 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 20 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 21 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 22 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 23 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 24 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 25 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 26 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 27 host: rich133-k40-22-l.pace.gatech.edu\n",
      "MPI proc 28 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 29 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 30 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 31 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 32 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 33 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 34 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 35 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 36 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 37 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 38 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 39 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 40 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 41 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 42 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 43 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 44 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 45 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 46 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 47 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 48 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 49 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 50 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 51 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 52 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 53 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 54 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI proc 55 host: rich133-k40-22-r.pace.gatech.edu\n",
      "MPI Point-to-point ping-pong test:\n",
      " # Processes  | Message Size | Total Size   | Time         | B/s\n",
      "           14              8            112   +7.77006e-07   +1.44143e+08\n",
      "           14             64            896   +4.83990e-07   +1.85128e+09\n",
      "           14            512           7168   +7.97033e-07   +8.99335e+09\n",
      "           14           4096          57344   +2.31814e-06   +2.47370e+10\n",
      "           14          32768         458752   +2.25320e-05   +2.03600e+10\n",
      "           14         262144        3670016   +1.01190e-04   +3.62686e+10\n",
      "           14        2097152       29360128   +6.13662e-04   +4.78441e+10\n",
      "           28              8            224   +2.97379e-06   +7.53246e+07\n",
      "           28             64           1792   +1.17183e-06   +1.52924e+09\n",
      "           28            512          14336   +4.43602e-06   +3.23173e+09\n",
      "           28           4096         114688   +7.35402e-06   +1.55953e+10\n",
      "           28          32768         917504   +4.54059e-05   +2.02067e+10\n",
      "           28         262144        7340032   +3.04597e-04   +2.40975e+10\n",
      "           28        2097152       58720256   +2.27786e-03   +2.57787e+10\n",
      "           56              8            448   +8.68392e-06   +5.15896e+07\n",
      "           56             64           3584   +5.93019e-06   +6.04366e+08\n",
      "           56            512          28672   +1.24280e-05   +2.30704e+09\n",
      "           56           4096         229376   +4.24411e-05   +5.40457e+09\n",
      "           56          32768        1835008   +2.82473e-04   +6.49622e+09\n",
      "           56         262144       14680064   +2.01774e-03   +7.27548e+09\n",
      "           56        2097152      117440512   +1.54734e-02   +7.58982e+09\n",
      "MPI Broadcast ping-pong test:\n",
      " # Processes  | Message Size | Total Size   | Time         | B/s\n",
      "           14              8            208   +2.53797e-06   +8.19554e+07\n",
      "           14             64           1664   +4.13489e-06   +4.02429e+08\n",
      "           14            512          13312   +7.24196e-06   +1.83818e+09\n",
      "           14           4096         106496   +2.67859e-05   +3.97583e+09\n",
      "           14          32768         851968   +1.84263e-04   +4.62365e+09\n",
      "           14         262144        6815744   +6.86264e-04   +9.93166e+09\n",
      "           14        2097152       54525952   +8.06049e-03   +6.76459e+09\n",
      "           28              8            432   +5.18799e-06   +8.32693e+07\n",
      "           28             64           3456   +4.10199e-06   +8.42518e+08\n",
      "           28            512          27648   +7.15899e-06   +3.86199e+09\n",
      "           28           4096         221184   +3.48599e-05   +6.34494e+09\n",
      "           28          32768        1769472   +7.75843e-04   +2.28071e+09\n",
      "           28         262144       14155776   +1.55229e-03   +9.11930e+09\n",
      "           28        2097152      113246208   +1.58986e-02   +7.12305e+09\n",
      "           56              8            880   +4.67205e-06   +1.88354e+08\n",
      "           56             64           7040   +5.68795e-06   +1.23770e+09\n",
      "           56            512          56320   +1.09310e-05   +5.15231e+09\n",
      "           56           4096         450560   +4.02708e-05   +1.11883e+10\n",
      "           56          32768        3604480   +2.12290e-04   +1.69790e+10\n",
      "           56         262144       28835840   +9.23447e-04   +3.12263e+10\n",
      "           56        2097152      230686720   +9.43590e-03   +2.44478e+10\n",
      "MPI Scatter ping-pong test:\n",
      " # Processes  | Message Size | Total Size   | Time         | B/s\n",
      "           14              8            208   +5.89013e-06   +3.53133e+07\n",
      "           14             64           1664   +6.79111e-06   +2.45026e+08\n",
      "           14            512          13312   +5.93901e-06   +2.24145e+09\n",
      "           14           4096         106496   +1.97990e-05   +5.37886e+09\n",
      "           14          32768         851968   +1.68452e-04   +5.05763e+09\n",
      "           28              8            432   +1.10271e-05   +3.91762e+07\n",
      "           28             64           3456   +1.03450e-05   +3.34075e+08\n",
      "           28            512          27648   +1.94170e-05   +1.42390e+09\n",
      "           28           4096         221184   +5.19412e-05   +4.25836e+09\n",
      "           28          32768        1769472   +4.10973e-04   +4.30557e+09\n",
      "           56              8            880   +1.43201e-05   +6.14519e+07\n",
      "           56             64           7040   +1.64521e-05   +4.27910e+08\n",
      "           56            512          56320   +4.51159e-05   +1.24834e+09\n",
      "           56           4096         450560   +1.12506e-04   +4.00476e+09\n",
      "           56          32768        3604480   +8.93289e-04   +4.03507e+09\n",
      "MPI All-reduce test:\n",
      " # Processes  | Message Size | Total Size   | Time         | B/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           14              8            208   +2.74301e-06   +7.58292e+07\n",
      "           14             64           1664   +4.14610e-06   +4.01341e+08\n",
      "           14            512          13312   +7.24006e-06   +1.83866e+09\n",
      "           14           4096         106496   +1.66779e-05   +6.38547e+09\n",
      "           14          32768         851968   +1.07024e-04   +7.96054e+09\n",
      "           14         262144        6815744   +7.29103e-04   +9.34812e+09\n",
      "           14        2097152       54525952   +3.41217e-03   +1.59798e+10\n",
      "           28              8            432   +6.17099e-06   +7.00050e+07\n",
      "           28             64           3456   +7.46918e-06   +4.62702e+08\n",
      "           28            512          27648   +1.57001e-05   +1.76101e+09\n",
      "           28           4096         221184   +1.15468e-04   +1.91554e+09\n",
      "           28          32768        1769472   +1.13918e-04   +1.55328e+10\n",
      "           28         262144       14155776   +7.16455e-04   +1.97581e+10\n",
      "           28        2097152      113246208   +3.91365e-03   +2.89362e+10\n",
      "           56              8            880   +6.65021e-06   +1.32327e+08\n",
      "           56             64           7040   +7.69687e-06   +9.14658e+08\n",
      "           56            512          56320   +1.84479e-05   +3.05293e+09\n",
      "           56           4096         450560   +4.87661e-05   +9.23920e+09\n",
      "           56          32768        3604480   +2.49291e-04   +1.44589e+10\n",
      "           56         262144       28835840   +1.11235e-03   +2.59232e+10\n",
      "           56        2097152      230686720   +4.36383e-03   +5.28633e+10\n",
      "MPI All-gather test:\n",
      " # Processes  | Message Size | Total Size   | Time         | B/s\n",
      "           14              8           1456   +5.30005e-06   +2.74715e+08\n",
      "           14             64          11648   +4.85897e-06   +2.39722e+09\n",
      "           14            512          93184   +7.58004e-06   +1.22933e+10\n",
      "           14           4096         745472   +3.43010e-05   +2.17332e+10\n",
      "           14          32768        5963776   +3.23522e-04   +1.84339e+10\n",
      "           28              8           6048   +4.85516e-06   +1.24569e+09\n",
      "           28             64          48384   +9.27806e-06   +5.21488e+09\n",
      "           28            512         387072   +2.09961e-05   +1.84354e+10\n",
      "           28           4096        3096576   +1.09057e-04   +2.83941e+10\n",
      "           28          32768       24772608   +6.85099e-04   +3.61592e+10\n",
      "           56              8          24640   +1.51951e-05   +1.62157e+09\n",
      "           56             64         197120   +1.29111e-05   +1.52675e+10\n",
      "           56            512        1576960   +4.65422e-05   +3.38824e+10\n",
      "           56           4096       12615680   +1.07602e-04   +1.17244e+11\n",
      "           56          32768      100925440   +1.32669e-03   +7.60732e+10\n",
      "MPI All-to-all test:\n",
      " # Processes  | Message Size | Total Size   | Time         | B/s\n",
      "           14              8           1352   +7.20191e-06   +1.87728e+08\n",
      "           14             64          10816   +2.03559e-05   +5.31344e+08\n",
      "           14            512          86528   +1.88050e-05   +4.60132e+09\n",
      "           14           4096         692224   +4.38299e-05   +1.57934e+10\n",
      "           14          32768        5537792   +3.57531e-04   +1.54890e+10\n",
      "           28              8           5832   +1.09231e-05   +5.33912e+08\n",
      "           28             64          46656   +6.88169e-05   +6.77973e+08\n",
      "           28            512         373248   +9.73902e-05   +3.83250e+09\n",
      "           28           4096        2985984   +2.00409e-04   +1.48995e+10\n",
      "           28          32768       23887872   +8.45265e-04   +2.82608e+10\n",
      "           56              8          24200   +1.52049e-04   +1.59159e+08\n",
      "           56             64         193600   +6.87557e-04   +2.81577e+08\n",
      "           56            512        1548800   +2.20071e-04   +7.03773e+09\n",
      "           56           4096       12390400   +1.73510e-03   +7.14103e+09\n",
      "           56          32768       99123200   +8.30150e-03   +1.19404e+10\n",
      "[mpiexec@rich133-k40-22-l.pace.gatech.edu] HYDT_bscd_pbs_wait_for_completion (../../../../src/pm/hydra/tools/bootstrap/external/pbs_wait.c:66): tm_poll(obit_event) failed with TM error 17002\n",
      "[mpiexec@rich133-k40-22-l.pace.gatech.edu] HYDT_bsci_wait_for_completion (../../../../src/pm/hydra/tools/bootstrap/src/bsci_wait.c:23): launcher returned error waiting for completion\n",
      "[mpiexec@rich133-k40-22-l.pace.gatech.edu] HYD_pmci_wait_for_completion (../../../../src/pm/hydra/pm/pmiserv/pmiserv_pmci.c:218): launcher returned error waiting for completion\n",
      "[mpiexec@rich133-k40-22-l.pace.gatech.edu] main (../../../../src/pm/hydra/ui/mpich/mpiexec.c:340): process manager error waiting for completion\n"
     ]
    },
    {
     "ename": "",
     "evalue": "255",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# please ignore the output here, I rerun with other nodes and have output in plot-input file\n",
    "make clean\n",
    "make\n",
    "mpirun -n 56 ./benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make runbenchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display < benchmarks.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Right now the graph is showing no values because the \"timing\" values are negative until you complete the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (2 pts)** We've talked in class about a simplified model of the cost of an MPI message: $\\lambda + g b$, where $\\lambda$ is the latency and $g$ is the inverse bandwidth.\n",
    "Using your graph for Send/Recv bandwidths for different message sizes (which was simply calculated from dividing the message size by the message time), estimate $\\lambda$ (units secs) and $g$ (units secs/byte) for this MPI implementation on these nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](benchmarks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](benchmarks2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| CommunicationType | g | $\\lambda$ |\n",
    "|---|---|---|\n",
    "| 'Send'| 2.22926832e-11| 4.79391442e-07|\n",
    "|'Bcast'|  4.22399938e-11| -4.86092968e-07|\n",
    "|'Allreduce'| 1.23547847e-11| 2.85296871e-07|\n",
    "|'Allgather'| 8.98445459e-12| -2.20375856e-09|\n",
    "|'Alltoall'| 3.26118493e-11| 4.52594649e-08|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (3 pts)** The point-to-point sends and receives in MPI are _symmetric_: there must be a receive for each send.  What if we start with a list of messages that is _asymmetric_: we know who to send to, but not who to receive from.\n",
    "\n",
    "Suppose rank $i$ has $N_i$ messages bound for receivers $r_{i,j}$ for $1 \\leq j \\leq N_i$.  Let $S_{i,j}$ be the size of the message from $i$ to $r_{i,j}$, and let $S_i = \\sum_j S_{i,j}$ be the total outgoing message volume from rank $i$.\n",
    "\n",
    "Process $r_{i,j}$ does not know it is going to receive a message from $i$.\n",
    "\n",
    "Give pseudocode below for two algorithms, using MPI sends, receives, and collectives that we have talked about, to send all of the messages.\n",
    "\n",
    "In the first one, assume that there is a large volume of communication: that $S_i \\in O(P)$ for each $i$.\n",
    "\n",
    "In the second one, assume that there is a small volume of communication, and a small number of communicators:\n",
    "$S_i \\in O(1)$, and each rank needs to send or receive a message from $O(1)$ other processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Message: (indexing is 1-based)\n",
    "```\n",
    "MPI_init()\n",
    "comm = communicator\n",
    "int commSize = size of communicator\n",
    "int rank = rank of current process\n",
    "int buffer[commSize] = {-1}\n",
    "for i = 1..commSize {\n",
    "    if rank == i {\n",
    "        buffer[i] = 0\n",
    "    }\n",
    "    if S[i,j] > 0 {\n",
    "        buffer[j] = encode(S[i,j], r[i,j]) // any possible way to save 2 piece data in one integer\n",
    "    }\n",
    "}\n",
    "MPI_Alltoall(MPI_IN_PLACE, 0, MPI_BYTE, buffer, numBytes, MPI_INT, comm);\n",
    "for i = 1..N[i]{\n",
    "    MPI_Send(message, S[i,j], MESSAGE_TYPE, r[i,j], i << 2 + j, comm) // any possible unique encoding of i,j\n",
    "}\n",
    "all_messages = Vector()\n",
    "for i = 1..commSize {\n",
    "    if buffer[i] != -1 {\n",
    "        message_size, from = decode(buffer[j])\n",
    "        int receive_buffer[message_size]\n",
    "        // tag here corresponds to tag in send, (from, i) is (i, j)\n",
    "        MPI_Recv(receive_buffer, message_size, MESSAGE_TYPE, from, from << 2 + i, comm)\n",
    "        all_messages.push(receive_buffer)\n",
    "    } \n",
    "}\n",
    "deal with all_messages\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant Message:\n",
    "```\n",
    "MPI_init()\n",
    "comm = communicator\n",
    "int commSize = size of communicator\n",
    "int rank = rank of current process\n",
    "int maxnumMessage\n",
    "MPI_Allreduce(N[i], maxnumMessage, MPI_INT, MPI_MAX, comm)\n",
    "\n",
    "int buffer[maxnumMessage]\n",
    "for i == 1..maxnumMessage {\n",
    "    buffer[i] = encode(S[rank, i], r[rank, i])\n",
    "}\n",
    "int* tmp_buffer\n",
    "if (!rank){\n",
    "    tmp_buffer = malloc(int*, maxnumMessage * commSize)\n",
    "}\n",
    "MPI_Gather(buffer, maxnumMessage, MPI_INT, tmp_buffer, maxnumMessage, MPI_INT, 0, comm)\n",
    "process_buffer(tmp_buffer)\n",
    "// process so that buffer for each comm contains who to receive from\n",
    "// here we assume #receive_from won't exceed maxnumMessage, if it did, scatter a new integer will fix it.\n",
    "MPI_Scatter(tmp_buffer, maxnumMessage, MPI_INT, buffer, maxnumMessage, MPI_INT, 0, comm)\n",
    "for i = 1..N[i]{\n",
    "    MPI_Send(message, S[i,j], MESSAGE_TYPE, r[i,j], i << 2 + j, comm) // any possible unique encoding of i,j\n",
    "}\n",
    "all_messages = Vector()\n",
    "for i = 1..maxnumMessage{\n",
    "    if (buffer[i] > 0){\n",
    "        message_size, from = decode(buffer[j])\n",
    "        int receive_buffer[message_size]\n",
    "        // tag here corresponds to tag in send, (from, i) is (i, j)\n",
    "        MPI_Recv(receive_buffer, message_size, MESSAGE_TYPE, from, from << 2 + i, comm)\n",
    "        all_messages.push(receive_buffer)\n",
    "    }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
