{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is meant to be run on **four nodes** with 28 cores.\n",
    "\n",
    "One thing we are trying to do is gauge the performance of a good MPI implementation and compare it to a good OpenMP implementation.  In order to do this, we will have to load some slightly different modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cse6230(4):ERROR:105: Unable to locate a modulefile for 'cse6230/gcc-omp-gnu'\n",
      "Currently Loaded Modulefiles:\n",
      "  1) curl/7.42.1\n"
     ]
    }
   ],
   "source": [
    "module unload cse6230\n",
    "module load cse6230/gcc-omp-gnu\n",
    "module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =========================================================================\n",
      "|                                                                         |\n",
      "|       A note about python/3.6:                                          |\n",
      "|       PACE is lacking the staff to install all of the python 3          |\n",
      "|       modules, but we do maintain an anaconda distribution for          |\n",
      "|       both python 2 and python 3. As conda significantly reduces        |\n",
      "|       the overhead with package management, we would much prefer        |\n",
      "|       to maintain python 3 through anaconda.                            |\n",
      "|                                                                         |\n",
      "|       All pace installed modules are visible via the module avail       |\n",
      "|       command.                                                          |\n",
      "|                                                                         |\n",
      " =========================================================================\n"
     ]
    }
   ],
   "source": [
    "module load cse6230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([mvapich](http://mvapich.cse.ohio-state.edu/) is a fork of the [mpich](https://www.mpich.org/) MPI implementation with some modifications for performance on various HPC hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring MPI primitives\n",
    "\n",
    "**Task 1 (3 pts)** The file `benchmarks.c` includes some basic benchmarks for measuring the performance of various MPI point-to-point and collective operations.  Right now, it is incomplete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2p pingpong / broadcast pingpong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /* TODO: split the communicator `comm` into one communicator for ranks\n",
      "  /* TODO: destroy the subcommunicator created in `splitCommunicator` */\n",
      "  /* TODO: Record the MPI walltime in `tic_p` */\n",
      "  /* TODO: Get the elapsed MPI walltime since `tic_in`,\n",
      "  /* TODO: take the times from all processes and compute the maximum,\n",
      "      // TODO: Set up a ping pong test for the broadcast collective.  When\n",
      "      // TODO: Set up a ping pong test for the scatter collective.  When\n",
      "      // TODO: Set up a timing loop for the following:\n",
      "      // TODO: Set up a timing loop for the following:\n",
      "      // TODO: Set up a timing loop for the following:\n"
     ]
    }
   ],
   "source": [
    "grep \"TODO\" benchmarks.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refering to a good [MPI Tutorial](https://computing.llnl.gov/tutorials/mpi/) or\n",
    "[lecture notes](http://vuduc.org/cse6230/slides/cse6230-fa14--06-mpi.pdf) as needed,\n",
    "fill in the missing MPI routines.\n",
    "\n",
    "Once you have done that, run the following script to generate a graph of benchmark bandwidths of MPI routines.  Note that these values are only for MPI messages within a node: values may be different when we start using multiple nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o benchmarks\n",
      "mpicc -fopenmp -I../../utils -g -Wall -O3 -std=c99 -c -o benchmarks.o benchmarks.c\n",
      "benchmarks.c: In function ‘main’:\n",
      "benchmarks.c:174: warning: unused variable ‘tic’\n",
      "mpicc -fopenmp -o benchmarks benchmarks.o \n",
      "MPI Version: 3.0\n",
      "Intel(R) MPI Library 5.1.1 for Linux* OS\n",
      "\n",
      "MPI # Procs: 28\n",
      "MPI Wtime 1.57117e+09, precision 1e-06\n",
      "MPI Wtime is global\n",
      "MPI proc 0 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 1 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 2 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 3 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 4 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 5 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 6 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 7 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 8 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 9 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 10 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 11 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 12 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 13 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 14 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 15 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 16 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 17 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 18 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 19 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 20 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 21 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 22 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 23 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 24 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 25 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 26 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI proc 27 host: rich133-h35-16-l.pace.gatech.edu\n",
      "MPI Point-to-point ping-pong test:\n",
      " # Processes  | Message Size | Total Size   | Time         | B/s\n",
      "           14              8            112   +1.72672e+09   +6.48629e-08\n",
      "           14             64            896   +1.72672e+09   +5.18903e-07\n",
      "           14            512           7168   +1.72672e+09   +4.15123e-06\n",
      "           14           4096          57344   +1.72672e+09   +3.32098e-05\n",
      "           14          32768         458752   +1.72672e+09   +2.65678e-04\n",
      "           14         262144        3670016   +1.72672e+09   +2.12543e-03\n",
      "           14        2097152       29360128   +1.72672e+09   +1.70034e-02\n",
      "           28              8            224   +1.72672e+09   +1.29726e-07\n",
      "           28             64           1792   +1.72672e+09   +1.03781e-06\n",
      "           28            512          14336   +1.72672e+09   +8.30245e-06\n",
      "           28           4096         114688   +1.72672e+09   +6.64196e-05\n",
      "           28          32768         917504   +1.72672e+09   +5.31357e-04\n",
      "           28         262144        7340032   +1.72672e+09   +4.25086e-03\n",
      "           28        2097152       58720256   +1.72672e+09   +3.40068e-02\n"
     ]
    }
   ],
   "source": [
    "make clean\n",
    "make\n",
    "mpirun -n 28 ./benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make runbenchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display < benchmarks.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Right now the graph is showing no values because the \"timing\" values are negative until you complete the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (2 pts)** We've talked in class about a simplified model of the cost of an MPI message: $\\lambda + g b$, where $\\lambda$ is the latency and $g$ is the inverse bandwidth.\n",
    "Using your graph for Send/Recv bandwidths for different message sizes (which was simply calculated from dividing the message size by the message time), estimate $\\lambda$ (units secs) and $g$ (units secs/byte) for this MPI implementation on these nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (3 pts)** The point-to-point sends and receives in MPI are _symmetric_: there must be a receive for each send.  What if we start with a list of messages that is _asymmetric_: we know who to send to, but not who to receive from.\n",
    "\n",
    "Suppose rank $i$ has $N_i$ messages bound for receivers $r_{i,j}$ for $1 \\leq j \\leq N_i$.  Let $S_{i,j}$ be the size of the message from $i$ to $r_{i,j}$, and let $S_i = \\sum_j S_{i,j}$ be the total outgoing message volume from rank $i$.\n",
    "\n",
    "Process $r_{i,j}$ does not know it is going to receive a message from $i$.\n",
    "\n",
    "Give pseudocode below for two algorithms, using MPI sends, receives, and collectives that we have talked about, to send all of the messages.\n",
    "\n",
    "In the first one, assume that there is a large volume of communication: that $S_i \\in O(P)$ for each $i$.\n",
    "\n",
    "In the second one, assume that there is a small volume of communication, and a small number of communicators:\n",
    "$S_i \\in O(1)$, and each rank needs to send or receive a message from $O(1)$ other processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
