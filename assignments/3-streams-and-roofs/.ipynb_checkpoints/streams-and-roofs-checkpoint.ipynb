{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streams and Roofs\n",
    "\n",
    "In this week's assignment we are going to make some roofline diagrams for some $n$-body problems.\n",
    "\n",
    "This week's assignment is meant to be run on a node with a Tesla P100 GPU.\n",
    "\n",
    "A reminder: when you are running a job to complete this week's assignment, you should make sure that you have requested exclusive access to a node, and that you have requested access to all CPU cores of this node.\n",
    "\n",
    "**Due: Thursday, September 12, before class**\n",
    "\n",
    "Let's load in our class module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "module use $CSE6230_DIR/modulefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =========================================================================\n",
      "|                                                                         |\n",
      "|       A note about python/3.6:                                          |\n",
      "|       PACE is lacking the staff to install all of the python 3          |\n",
      "|       modules, but we do maintain an anaconda distribution for          |\n",
      "|       both python 2 and python 3. As conda significantly reduces        |\n",
      "|       the overhead with package management, we would much prefer        |\n",
      "|       to maintain python 3 through anaconda.                            |\n",
      "|                                                                         |\n",
      "|       All pace installed modules are visible via the module avail       |\n",
      "|       command.                                                          |\n",
      "|                                                                         |\n",
      " =========================================================================\n"
     ]
    }
   ],
   "source": [
    "module load cse6230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\n",
      "  1) curl/7.42.1\n",
      "  2) git/2.13.4\n",
      "  3) python/3.6\n",
      "  4) /nv/coc-ice/tisaac3/opt/pace-ice/modulefiles/jupyter/1.0\n",
      "  5) intel/16.0\n",
      "  6) cuda/8.0.44\n",
      "  7) /nv/coc-ice/tisaac3/opt/pace-ice/modulefiles/hpctoolkit/2018.18\n",
      "  8) impi/5.1.1.109\n",
      "  9) cse6230/core(default)\n"
     ]
    }
   ],
   "source": [
    "module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And verify that we're running where we expect to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Now, about the $n$-body simulations we're going to run: a classical $n$-body simulation has each body, or *particle*, interacting with each other, for $n(n+1)/2$ total interactions.  That hardly matches up to the streaming kernels we've been talking about!  So we're going to simplify a bit.\n",
    "\n",
    "We are going to simulate $n$ infinitesimal particles circling around an infinitely massive sun at the origin.  In this system, the sun is unmoved, and the particles are not affected by each other.\n",
    "\n",
    "We're going to normalize our coefficients and say that each particle is an ordinary differential equation with *six* components: three of position $X=(x, y, z)$ and three of velocity $U=(u, v, w)$.  The position, is changed by the velocity, of course, but the velocity changes under acceleration that depends on position:\n",
    "\n",
    "$$\\begin{aligned} \\dot{X} &= V \\\\ \\dot{V} &= - \\frac{X}{|X|^3}.\\end{aligned}$$\n",
    "\n",
    "To discretize this differential equation, we are going to use a time stepping method called the Verlet leap-frog method, which is good for calculating long simulations of stable orbits.  Given a time step length `dt`, our pseudocode for one time step for one particle looks like the following:\n",
    "\n",
    "1. `X += 0.5 * dt * V`\n",
    "2. `R2 = X . X` (dot product)\n",
    "3. `R = sqrt (R2)`\n",
    "4. `IR3 = 1. / (R2 * R)`\n",
    "5. `V -= X * dt * IR3`\n",
    "6. `X += 0.5 * dt * V`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Assuming `sqrt` and `div` count for one flop each, and assuming `x, y, z` and `u, v, w` are **double-precision** floating point\n",
    "numbers, **estimate the arithmetic intensity of a *particle time step***.  You should ignore the time it takes to load `dt`.  Your answer should have units of flops / byte.  Give your answer in a new cell below this one, and show how you arrived at that number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{7}{24}$ flops/byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose our compiler is smart enough to keep reused value in register\n",
    "\n",
    "1. `X += 0.5 * dt * V`: 1 flop (`0.5 * dt`) + 3 flops (`k * V`) + 3 (`+`) = 7 flops; read 6 variables\n",
    "2. `R2 = X . X` (dot product): 5 flops\n",
    "3. `R = sqrt (R2)`: 1 flop\n",
    "4. `IR3 = 1. / (R2 * R)`: 2 flops\n",
    "5. `V -= X * dt * IR3`: 7 flops\n",
    "6. `X += 0.5 * dt * V`: 6 flops; write 6\n",
    "\n",
    "in total 28 flops\n",
    "\n",
    "each `double` variable is 8 bytes, in total 12 times r/w, 96 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Using the peak theoretical **double-precision** flop/s of this node (flop/s on the CPUs and GPU combined), calculated the same way as in the last assignment, and reported peak memory bandwidths from the manufacturers, **estimate the system balance of CPUs and the GPU of this node separately**.  Note that the bandwidth estimate from intel will be for one socket (4 cores) with attached memory, and our node has two such sockets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core id\t\t: 0\n",
      "core id\t\t: 1\n",
      "core id\t\t: 2\n",
      "core id\t\t: 3\n",
      "core id\t\t: 0\n",
      "core id\t\t: 1\n",
      "core id\t\t: 2\n",
      "core id\t\t: 3\n"
     ]
    }
   ],
   "source": [
    "cat /proc/cpuinfo | grep \"core id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cpu peak flop: 8 * 2.6G * 16 (double precision) = 332 Gflop/s\n",
    "\n",
    "gpu peak flop: reported by nvidia, 4700 Gflop/s\n",
    "\n",
    "cpu peak bandwidth: 68.3 GB/s \n",
    "\n",
    "gpu peak bandwidth: 32 Gb/s (PCIe), 732GB/s (HBM)\n",
    "\n",
    "cpu balance point: 332/68.3 = 4.88 flop/byte\n",
    "\n",
    "gpu balance point: 146.88 flop/byte(PCIe), 6.42 flop/byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we didn't take the peak flop/s values from the manufacturers at face value, and this week we are not going to take the peak Gbyte/s for granted either.  Last week we used a custom benchmark in our calculations; this week we will use an industry standard: the\n",
    "[STREAM benchmark](https://www.cs.virginia.edu/stream/ref.html).\n",
    "\n",
    "We can run the stream benchmark on the CPUs for this assignment with a makefile target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OMP_NUM_THREADS=8 make runstream STREAM_N=40000 COPTFLAGS=\"-O3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `STREAM_N` argument will control the size of the stream arrays.\n",
    "\n",
    "**Question 3:** Modify the invocation of `make runstreams` by modifying the values of\n",
    "`STREAM_N`, `COPTFLAGS` (optimization flags), `OMP_NUM_THREADS` and/or `OMP_PROC_BIND` (the openMP environment variables) to get the largest streaming bandwidth from main memory that you can for this node.\n",
    "\n",
    "[The OpenMP environment variables were not defined by me in the Makefile: they are environment variables that will be detected by the OpenMP runtime when an OpenMP program begins.  You should put them _before_ the make command, e.g. `OMP_NUM_THREADS=5 make runstream STREAM_N=40000000`]\n",
    "\n",
    "- Follow the directions in the output of the file and make sure you are testing streaming bandwidth from memory and not from a higher level of cache.\n",
    "- You should try to get close to the same bandwidth for all tests:\n",
    "\n",
    "- There are two variables in the openMP environment you should care about, OMP_NUM_THREADS, which is self explanatory, and OMP_PROC_BIND is discussed [here](http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html).  **You should try to use as few threads as possible** to achieve peak bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OMP_PROC_BIND: \n",
    "- `TRUE`: Enable process binding.\n",
    "- `FALSE`: Disable process binding.\n",
    "- `MASTER`: Bind threads to the same place as the master thread.\n",
    "- `CLOSE`: Bind threads close to the master thread while still distributing threads for load balancing.\n",
    "- `SPREAD`: Bind threads as evenly distributed (spread) as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in {3..10}\n",
    "do\n",
    "echo $i\n",
    "OMP_NUM_THREADS=$i\n",
    "OMP_PROC_BIND=SPREAD\n",
    "# unset OMP_PROC_BIND\n",
    "make clean\n",
    "make runstream STREAM_N=40000000 COPTFLAGS=\"-O3\" | grep -2 Scale:\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o cloud stream stream2 streamcu streamcu2\n",
      "icc -g -Wall -fPIC -O3 -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -o stream stream.c -DSTREAM_ARRAY_SIZE=40000000\n",
      "./stream\n",
      "-------------------------------------------------------------\n",
      "STREAM version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 40000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 305.2 MiB (= 0.3 GiB).\n",
      "Total memory required = 915.5 MiB (= 0.9 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "-------------------------------------------------------------\n",
      "Number of Threads requested = 8\n",
      "Number of Threads counted = 8\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 14122 microseconds.\n",
      "   (= 14122 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:           49184.7     0.013647     0.013012     0.015488\n",
      "Scale:          50176.7     0.013639     0.012755     0.017021\n",
      "Add:            60385.0     0.018737     0.015898     0.021769\n",
      "Triad:          54759.7     0.018950     0.017531     0.019369\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-13 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "OMP_NUM_THREADS=8\n",
    "OMP_PROC_BIND=SPREAD\n",
    "make clean\n",
    "make runstream STREAM_N=40000000 COPTFLAGS=\"-O3\"\n",
    "# 4 chaanels/socket * 2 sockets giving 8 channels\n",
    "# seems value around 6,7,8 is not so stable, 7 gives best value for most times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** What does `OMP_PROC_BIND=close` mean, and why is it a bad choice, not just for this benchmark, but for any streaming kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The value OMP_PROC_BIND=close means that the assignment goes successively through the available places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** I've modified the benchmark, calling it `stream2.c`.  Here's the difference, it's one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267d266\n",
      "< #pragma omp parallel for\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "diff stream.c stream2.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your options for `runstream` to `runstream2` below.  The reported results should be different: why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o cloud stream stream2 streamcu streamcu2\n",
      "icc -g -Wall -fPIC -O3 -xHost -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -o stream2 stream2.c -DSTREAM_ARRAY_SIZE=10000000\n",
      "./stream2\n",
      "-------------------------------------------------------------\n",
      "STREAM version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 10000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 76.3 MiB (= 0.1 GiB).\n",
      "Total memory required = 228.9 MiB (= 0.2 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "-------------------------------------------------------------\n",
      "Number of Threads requested = 8\n",
      "Number of Threads counted = 8\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 6092 microseconds.\n",
      "   (= 6092 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:           32154.1     0.007921     0.004976     0.012213\n",
      "Scale:          31917.1     0.008160     0.005013     0.012629\n",
      "Add:            35015.8     0.009190     0.006854     0.014405\n",
      "Triad:          35134.3     0.009146     0.006831     0.012080\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-13 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "OMP_NUM_THREADS=8\n",
    "OMP_PROC_BIND=SPREAD\n",
    "make clean\n",
    "make runstream2 STREAM_N=10000000 COPTFLAGS=\"-O3 -xHost\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result goes half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Now we're going to run stream benchmarks for the GPU.  As above, modify the array size until you believe you are testing streaming bandwidth from memory and not from cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in {1..20..4}\n",
    "do\n",
    "echo $i\n",
    "OMP_NUM_THREADS=$i\n",
    "# OMP_PROC_BIND=FALSE\n",
    "unset OMP_PROC_BIND\n",
    "make clean\n",
    "make runstreamcu STREAM_N=10000000  CUOPTFLAGS=\"-O3\" | grep -2 Scale:\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "rm -f *.o cloud stream stream2 streamcu streamcu2\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:           20336.0     0.000009     0.000008     0.000010\n",
      "Scale:          20336.0     0.000009     0.000008     0.000009\n",
      "Add:            30504.0     0.000012     0.000008     0.000031\n",
      "Triad:          27206.3     0.000011     0.000009     0.000024\n",
      "100000\n",
      "rm -f *.o cloud stream stream2 streamcu streamcu2\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:          181375.3     0.000010     0.000009     0.000011\n",
      "Scale:         181375.3     0.000010     0.000009     0.000011\n",
      "Add:           245520.2     0.000010     0.000010     0.000011\n",
      "Triad:         264903.4     0.000010     0.000009     0.000011\n",
      "1000000\n",
      "rm -f *.o cloud stream stream2 streamcu streamcu2\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:          422068.3     0.000039     0.000038     0.000040\n",
      "Scale:         422068.3     0.000039     0.000038     0.000042\n",
      "Add:           453438.3     0.000053     0.000053     0.000054\n",
      "Triad:         461758.2     0.000052     0.000052     0.000053\n",
      "10000000\n",
      "rm -f *.o cloud stream stream2 streamcu streamcu2\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:          530085.8     0.000303     0.000302     0.000305\n",
      "Scale:         528000.5     0.000304     0.000303     0.000304\n",
      "Add:           545600.5     0.000440     0.000440     0.000441\n",
      "Triad:         545600.5     0.000441     0.000440     0.000444\n"
     ]
    }
   ],
   "source": [
    "unset OMP_NUM_THREADS\n",
    "\n",
    "for i in {4..7}\n",
    "do\n",
    "echo $(bc <<< \"10^$i\")\n",
    "make clean\n",
    "make runstreamcu STREAM_N=$(bc <<< \"10^$i\")  CUOPTFLAGS=\"-O3\" | grep -2 Scale:\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 (2 pts):** This is final time we're running a stream benchmark, I promise.  This benchmark is also for the GPU, but instead of the arrays originating in the GPUs memory, they start on the CPUs memory, and must be transfered to the GPU and back.  This mimics a common design pattern when people try to modify their code for GPUs: identify the bottleneck kernel, and try to \"offload\" it to the GPU, where it will have a higher throughput (once it get's there).  You don't have to modify this run, I just want you to see what bandwidths it reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -ccbin=icpc -lineinfo -Xcompiler '-fPIC' -O -o streamcu2 stream2.cu -DSTREAM_ARRAY_SIZE=1000000\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "stream2.cu(569): warning: variable \"i\" was set but never used\n",
      "\n",
      "stream2.cu(569): warning: variable \"i\" was set but never used\n",
      "\n",
      "./streamcu2\n",
      "-------------------------------------------------------------\n",
      "CSE6230 CUDA STREAM based on version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 1000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 7.6 MiB (= 0.0 GiB).\n",
      "Total memory required = 22.9 MiB (= 0.0 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "Ordinal of GPUs requested = 0\n",
      "  Device name: Tesla P100-PCIE-16GB\n",
      "  Memory Clock Rate (KHz): 715000\n",
      "  Memory Bus Width (bits): 4096\n",
      "  Peak Memory Bandwidth (GB/s): 732.160000\n",
      "\n",
      "-------------------------------------------------------------\n",
      "1.000000 2.000000 0.000000\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 1351 microseconds.\n",
      "   (= 1351 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:            6128.1     0.002623     0.002611     0.002638\n",
      "Scale:           6128.1     0.002618     0.002611     0.002634\n",
      "Add:             6148.9     0.003923     0.003903     0.003952\n",
      "Triad:           6142.9     0.003926     0.003907     0.003973\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-12 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "make runstreamcu2 STREAM_N=1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the three peak bandwidths that we have *computed* (not the reported values from question 2) -- CPU, GPU with arrays on the GPU, and GPU with arrays on the CPU -- and with the theoretical peak flop/s for the CPU and GPU, compute *effective system balances* and create a plot with rooflines for all three balances overlayed.\n",
    "\n",
    "- The y axis should be absolute Gflop/s, not relative, so we can compare them, and should be labeled \"Gflop/s\"\n",
    "- Label with roofline goes with which balance: \"CPU\", \"GPU\", \"CPU->GPU->CPU\"\n",
    "- The x axis should be in units of \"double precision flops / byte\"\n",
    "\n",
    "Save your plot as the jpg `threerooflines.jpg` so that it can embed in the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU bandwith: 56GB/s\n",
    "GPU bandwith: 6GB/s(PCIe), 530GB/s(HBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Three rooflines](./threerooflines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8 (2 pts):** Remember those particles all the way back in question 1?  Your arithmetic intensity estimate could be placed on the roofline plot for the CPUs, and you could make a judgement about whether the kernel is compute bound or memory bound.\n",
    "\n",
    "Now let's put it to the test.  The `make runcloud` target simulates `NPOINT` particles orbiting the sun for `NT` time steps.  Because these particles are independent, you can optionally \"chunk\" multiple time steps for each particle independent of the other particles.  Doing this reduces the number of memory accesses per flop:  each particle stays in register for `NCHUNK` time steps.\n",
    "\n",
    "Do your best to optimize the throughput of the simulation both in the limit of few particles and many time steps, and in the limit of many particles and few time steps.\n",
    "Do that by modifying the commands below.\n",
    "\n",
    "- Make the simulations each run about a second\n",
    "- Do your best to optimize the compiler flags and the runtime (openMP) environment\n",
    "\n",
    "Using the outputs of those runs, estimate the floating point efficiency of our particle-time-step kernel: compare the peak flop/s of the CPU, to the product of particle time steps per second and your estimate of the flops per particle time step. and divide by the throughput of particle time steps per second.  Give that effective arithmetic intensity below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See above for the plot. We can see that the particle problem is compute bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============NUM THREADS: 21  CHUNK: 10000\n",
      "[./cloud]: 1.408326e+00 elapsed seconds\n",
      "[./cloud]: 4.544403e+08 particle time steps per second\n",
      "[./cloud]: 4.544403e+04 particle time step chunks per second\n"
     ]
    }
   ],
   "source": [
    "# max 5e8\n",
    "OMP_PROC_BIND=SPREAD\n",
    "for chunk in 4\n",
    "do\n",
    "    for numthreads in 21\n",
    "    do\n",
    "        OMP_NUM_THREADS=$numthreads\n",
    "        echo \"===============NUM THREADS: $numthreads  CHUNK: $(bc <<< \"10^$chunk\")\"\n",
    "        make clean > /dev/null\n",
    "        make runcloud NPOINT=64 NT=10000000 NCHUNK=$(bc <<< \"10^$chunk\") COPTFLAGS=\"-O3\" | grep \"/cloud]:\"\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5e8 time_steps per sec(see above) * 28 flops per step(see Q1) = 14e9 flops per sec\n",
    "\n",
    "cpu peak: 300e9 flops per sec\n",
    "\n",
    "effiency: roughly 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============NUM THREADS: 6400  CHUNK: 1\n",
      "[./cloud]: 6.605151e-01 elapsed seconds\n",
      "[./cloud]: 9.689408e+08 particle time steps per second\n",
      "[./cloud]: 9.689408e+08 particle time step chunks per second\n",
      "===============NUM THREADS: 640000  CHUNK: 1\n",
      "OMP: Error #34: System unable to allocate necessary resources for OMP thread:\n",
      "OMP: System error #11: Resource temporarily unavailable\n",
      "OMP: Hint: Try decreasing the value of OMP_NUM_THREADS.\n",
      "make: *** [runcloud] Aborted\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# max: 9e8\n",
    "OMP_PROC_BIND=SPREAD\n",
    "for chunk in 1\n",
    "do\n",
    "    for numthreads in 6400\n",
    "    do\n",
    "        OMP_NUM_THREADS=$numthreads\n",
    "        echo \"===============NUM THREADS: $numthreads  CHUNK: $chunk\"\n",
    "        make clean > /dev/null\n",
    "        make runcloud NPOINT=6400000 NT=100 NCHUNK=$chunk COPTFLAGS=\"-O3\" | grep \"/cloud]:\"\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9e8 time_steps per sec(see above) * 28 flops per step(see Q1) = 28e9 flops per sec\n",
    "\n",
    "cpu peak: 300e9 flops per sec\n",
    "\n",
    "effiency: roughly 10% in this case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
